# -*- coding: utf-8 -*-
"""Newspaper_Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tubFJ4GpiRcksvJWbiNVXd3B5Whv-RnJ
"""

!pip install PyPDF2

import PyPDF2

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text += page.extract_text()
    return text

# Example usage:
pdf_path = '/content/newspaper1.pdf'
raw_text = extract_text_from_pdf(pdf_path)
print(raw_text[:1000])  # Print the first 500 characters to verify

!pip install nltk

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    # Lowercasing
    text = text.lower()
    # Remove non-textual data
    text = re.sub(r'\d+', '', text)  # Remove digits
    text = re.sub(r'\W+', ' ', text)  # Remove special characters
    # Tokenization
    sentences = sent_tokenize(text)
    words = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    return " ".join(words), sentences

# Example usage:
processed_text, sentences = preprocess_text(raw_text)
print(processed_text[:500])  # Print the first 500 characters to verify

!pip install transformers
!pip install torch

from transformers import pipeline

def split_into_paragraphs(text):
    paragraphs = text.split("\n")  # Assuming paragraphs are separated by double newlines
    return [para for para in paragraphs if len(para.strip()) > 0]  # Remove empty paragraphs

# Example usage:
paragraphs = split_into_paragraphs(raw_text)
print(len(paragraphs))

from collections import defaultdict

categories = ["Politics", "Sports", "Entertainment", "Business", "Technology", "Health", "Science"]

classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")


def classify_text(text):
    results = classifier(text, candidate_labels=categories, multi_label=False)
    return results['labels'][0]  # Return the top category

def classify_paragraphs(paragraphs):
    categorized_paragraphs = defaultdict(list)

    for paragraph in paragraphs:
        category = classify_text(paragraph)
        categorized_paragraphs[category].append(paragraph)

    return categorized_paragraphs

# Example usage:
categorized_paragraphs = classify_paragraphs(paragraphs)

!pip install bert-extractive-summarizer

from summarizer import Summarizer

def bert_extractive_summary(text):
    model = Summarizer()
    summary = model(text, ratio=0.1)  # Summarize to 10% of the original text
    return summary

# # Example usage:
# summary_text = bert_extractive_summary(raw_text)
# print(summary_text)

def summarize_by_category(categorized_paragraphs, summary_func):
    categorized_summaries = {}

    for category, paragraphs in categorized_paragraphs.items():
        combined_text = " ".join(paragraphs)
        summary = summary_func(combined_text)
        categorized_summaries[category] = summary

    return categorized_summaries

# Example usage:
categorized_summaries = summarize_by_category(categorized_paragraphs, bert_extractive_summary)

!pip install pdfkit
!apt-get install -y wkhtmltopdf

import pdfkit

# def generate_pdf_with_utf8(summary, output_path):
#     html_content = """
#     <html>
#     <body>
#     <p style="font-family: Arial; font-size: 12px;">
#     """ + summary.replace('\n', '<br>') + """
#     </p>
#     </body>
#     </html>
#     """
#     pdfkit.from_string(html_content, output_path)

# # Example usage:
# generate_pdf_with_utf8(summary_text, "/content/summary.pdf")

def generate_categorized_pdf(categorized_summaries, output_path):
    html_content = "<html><body>"

    for category, summaries in categorized_summaries.items():
        html_content += f"<h2>{category}</h2>"
        for summary in summaries:
            html_content += f"{summary}"
        html_content += "<hr>"  # Add a horizontal line to separate categories

    html_content += "</body></html>"

    pdfkit.from_string(html_content, output_path)

# Example usage:
generate_categorized_pdf(categorized_summaries, "/content/summary.pdf")